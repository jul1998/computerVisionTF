{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-yaLCje5pWXT"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "3LGZZfXXpavo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d moltean/fruits"
      ],
      "metadata": {
        "id": "JxfK-ihspatg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir\n"
      ],
      "metadata": {
        "id": "2sTYgGc8parZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip data\n",
        "unzip_data('/content/fruits.zip')\n"
      ],
      "metadata": {
        "id": "XrLbh0_6papQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir('/content/fruits-360_dataset')"
      ],
      "metadata": {
        "id": "wXLDaKiqpanQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/fruits-360_dataset/fruits-360/Training'\n",
        "test_dir = '/content/fruits-360_dataset/fruits-360/Test'\n"
      ],
      "metadata": {
        "id": "Z18EXIARtVMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select 10% of data"
      ],
      "metadata": {
        "id": "nAj9yT5I3CIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Set the percentage of data to select (1% in this case)\n",
        "percentage = 10\n",
        "\n",
        "def select_percentage_of_data(src_dir, dest_dir, percentage):\n",
        "    # Get the list of subdirectories (each subdirectory represents a fruit category)\n",
        "    categories = os.listdir(src_dir)\n",
        "\n",
        "    # Create the destination directory if it doesn't exist\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate over each fruit category\n",
        "    for category in categories:\n",
        "        category_src_dir = os.path.join(src_dir, category)\n",
        "        category_dest_dir = os.path.join(dest_dir, category)\n",
        "\n",
        "        # Get the list of all files in the current category\n",
        "        all_files = os.listdir(category_src_dir)\n",
        "\n",
        "        # Calculate the number of files to select based on the percentage\n",
        "        num_files_to_select = int(len(all_files) * (percentage / 100))\n",
        "\n",
        "        # Randomly select files\n",
        "        selected_files = random.sample(all_files, num_files_to_select)\n",
        "\n",
        "        # Create the destination directory for the current category\n",
        "        os.makedirs(category_dest_dir, exist_ok=True)\n",
        "\n",
        "        # Copy selected files to the destination directory\n",
        "        for file in selected_files:\n",
        "            src_path = os.path.join(category_src_dir, file)\n",
        "            dest_path = os.path.join(category_dest_dir, file)\n",
        "            shutil.copy(src_path, dest_path)\n",
        "\n",
        "# Specify the directories\n",
        "selected_train_dir = '/content/selected_fruits_dataset/Training'\n",
        "selected_test_dir = '/content/selected_fruits_dataset/Test'\n",
        "\n",
        "# Specify the original train and test directories\n",
        "train_dir = '/content/fruits-360_dataset/fruits-360/Training'\n",
        "test_dir = '/content/fruits-360_dataset/fruits-360/Test'\n",
        "\n",
        "# Select 1% of the data for training\n",
        "select_percentage_of_data(train_dir, selected_train_dir, percentage)\n",
        "\n",
        "# Select 1% of the data for testing\n",
        "select_percentage_of_data(test_dir, selected_test_dir, percentage)\n"
      ],
      "metadata": {
        "id": "lVBlqs2Ey1eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir('/content/selected_fruits_dataset')"
      ],
      "metadata": {
        "id": "f_fvGJL9y1bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_10_percent = '/content/selected_fruits_dataset/Training'\n",
        "test_dir_10_percent = '/content/selected_fruits_dataset/Test'"
      ],
      "metadata": {
        "id": "n0WwuiA4y1Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data inputs\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "\n",
        "\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n",
        "                                                                                label_mode=\"categorical\",\n",
        "                                                                                image_size=IMG_SIZE,\n",
        "                                                                 shuffle=True)\n",
        "\n",
        "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(test_dir_10_percent,\n",
        "                                                                label_mode=\"categorical\",\n",
        "                                                                image_size=IMG_SIZE,\n",
        "                                                                shuffle=False) # don't shuffle test data for prediction analysis\n"
      ],
      "metadata": {
        "id": "iMZE12BDsUOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View images"
      ],
      "metadata": {
        "id": "KqEiZkML3GIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import random\n",
        "\n",
        "def display_random_image_grid_with_labels(folder_path, num_rows, num_cols):\n",
        "    \"\"\"\n",
        "    Display a grid of randomly selected images from subfolders with labels.\n",
        "\n",
        "    Parameters:\n",
        "    - folder_path (str): Path to the folder containing subfolders with images.\n",
        "    - num_rows (int): Number of rows in the grid.\n",
        "    - num_cols (int): Number of columns in the grid.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
        "\n",
        "    # Get a list of subfolders (class labels)\n",
        "    class_labels = [subfolder for subfolder in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, subfolder))]\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            class_label = random.choice(class_labels)\n",
        "            class_folder_path = os.path.join(folder_path, class_label)\n",
        "\n",
        "            # Get a list of image files in the selected subfolder\n",
        "            image_files = [f for f in os.listdir(class_folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "            if image_files:\n",
        "                # Shuffle the list of image files\n",
        "                random.shuffle(image_files)\n",
        "\n",
        "                # Select a random image to display\n",
        "                img_path = os.path.join(class_folder_path, random.choice(image_files))\n",
        "                img = mpimg.imread(img_path)\n",
        "                axes[i, j].imshow(img)\n",
        "                axes[i, j].axis('off')\n",
        "                axes[i, j].set_title(class_label, fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "folder_path = '/content/selected_fruits_dataset/Training'\n",
        "num_rows = 2\n",
        "num_cols = 6\n",
        "\n",
        "display_random_image_grid_with_labels(folder_path, num_rows, num_cols)\n"
      ],
      "metadata": {
        "id": "WpRVvzuXsUMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a checkpoint"
      ],
      "metadata": {
        "id": "bLkyTvQ35dIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a checkpoint callback to save model\n",
        "checkpoint_path = 'fruits_data_model_checkpoint'\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "KTHwnYUksUJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create data augmentation layer"
      ],
      "metadata": {
        "id": "52R4_dlB5omZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modules for model creation\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Setup data augmentation\n",
        "data_augmentation = Sequential([\n",
        "  preprocessing.RandomFlip(\"horizontal\"), # randomly flip images on horizontal edge\n",
        "  preprocessing.RandomRotation(0.2), # randomly rotate images by a specific amount\n",
        "  preprocessing.RandomHeight(0.2), # randomly adjust the height of an image by a specific amount\n",
        "  preprocessing.RandomWidth(0.2), # randomly adjust the width of an image by a specific amount\n",
        "  preprocessing.RandomZoom(0.2), # randomly zoom into an image\n",
        "  # preprocessing.Rescaling(1./255) # keep for models like ResNet50V2, remove for EfficientNet\n",
        "], name=\"data_augmentation\")"
      ],
      "metadata": {
        "id": "-ozCECwx5gES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up a base model"
      ],
      "metadata": {
        "id": "iWdpAf1b53Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_data_10_percent.class_names)\n",
        "num_classes"
      ],
      "metadata": {
        "id": "7oSv-G2k7FRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Setup model architecture with trianable top layers\n",
        "inputs = layers.Input(shape=(224, 224, 3), name='input_layer')\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D(name='global_average_pooling')(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax', name='output_layer')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "_8dYx6Ml5gBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "5guFyNUv5f_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(), # use Adam with default settings\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit\n",
        "history_all_classes_10_percent = model.fit(train_data_10_percent,\n",
        "                                           epochs=5, # fit for 5 epochs to keep experiments quick\n",
        "                                           validation_data=test_data_10_percent,\n",
        "                                           validation_steps= len(test_data_10_percent), # evaluate on smaller portion of test data\n",
        "                                           callbacks=[checkpoint_callback]) # save best model weights to file\n"
      ],
      "metadata": {
        "id": "PFd61VyS5f86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_all_classes_10_percent)"
      ],
      "metadata": {
        "id": "w1xBRHO75f6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pred_probs_10_percent = model.predict(test_data_10_percent)\n",
        "model_pred_probs_10_percent.shape"
      ],
      "metadata": {
        "id": "z32EJ0VyMWli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do they look?\n",
        "model_pred_probs_10_percent[:10]"
      ],
      "metadata": {
        "id": "UV-V-sQOMWjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class predicitons of each label\n",
        "pred_classes_10_percent = model_pred_probs_10_percent.argmax(axis=1)\n",
        "\n",
        "# How do they look?\n",
        "pred_classes_10_percent[:100]\n"
      ],
      "metadata": {
        "id": "bsm1TjPCMWct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_labels_10_percent = []\n",
        "for images, labels in test_data_10_percent.unbatch(): # unbatch the test data and get images and labels\n",
        "  y_labels_10_percent.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)\n",
        "y_labels_10_percent[:10] # check what they look like (unshuffled)"
      ],
      "metadata": {
        "id": "SNmaaA3LMWay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import calculate_results,\n",
        "model_0_results = calculate_results(y_labels_10_percent,pred_classes_10_percent )\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "nOqftbMfBXVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes = train_data_10_percent.class_names\n",
        "train_classes"
      ],
      "metadata": {
        "id": "IfysCRDFOeoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: The following confusion matrix code is a remix of Scikit-Learn's\n",
        "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):\n",
        "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
        "\n",
        "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
        "  will be used.\n",
        "\n",
        "  Args:\n",
        "    y_true: Array of truth labels (must be same shape as y_pred).\n",
        "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
        "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
        "    figsize: Size of output figure (default=(10, 10)).\n",
        "    text_size: Size of output figure text (default=15).\n",
        "    norm: normalize values or not (default=False).\n",
        "    savefig: save confusion matrix to file (default=False).\n",
        "\n",
        "  Returns:\n",
        "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
        "\n",
        "  Example usage:\n",
        "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
        "                          y_pred=y_preds, # predicted labels\n",
        "                          classes=class_names, # array of class label names\n",
        "                          figsize=(15, 15),\n",
        "                          text_size=10)\n",
        "  \"\"\"\n",
        "  # Create the confustion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "\n",
        "  # Plot the figure and make it pretty\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  # Are there a list of classes?\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "\n",
        "  # Label the axes\n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes),\n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "\n",
        "  # Make x-axis labels appear on bottom\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  ### Added: Rotate xticks for readability & increase font size (required due to such a large confusion matrix)\n",
        "  plt.xticks(rotation=70, fontsize=text_size)\n",
        "  plt.yticks(fontsize=text_size)\n",
        "\n",
        "  # Set the threshold for different colors\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  # Plot the text on each cell\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    if norm:\n",
        "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "    else:\n",
        "      plt.text(j, i, f\"{cm[i, j]}\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "\n",
        "  # Save the figure to the current working directory\n",
        "  if savefig:\n",
        "    fig.savefig(\"confusion_matrix.png\")"
      ],
      "metadata": {
        "id": "el1dZJ6F5f4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(\n",
        "    y_labels_10_percent,\n",
        "    pred_classes_10_percent,\n",
        "    train_classes,\n",
        "    figsize=(50,50)\n",
        ")"
      ],
      "metadata": {
        "id": "pwo_yLv3sUIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)\n",
        "classification_report_dict\n"
      ],
      "metadata": {
        "id": "8mASPRn6sUFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty dictionary\n",
        "class_f1_scores = {}\n",
        "# Loop through classification report items\n",
        "for k, v in classification_report_dict.items():\n",
        "  if k == \"accuracy\": # stop once we get to accuracy key\n",
        "    break\n",
        "  else:\n",
        "    # Append class names and f1-scores to new dictionary\n",
        "    class_f1_scores[train_classes[int(k)]] = v[\"f1-score\"]\n",
        "class_f1_scores, len(class_f1_scores)"
      ],
      "metadata": {
        "id": "Yyz3Gm4ZRVL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn f1-scores into dataframe for visualization\n",
        "import pandas as pd\n",
        "f1_scores = pd.DataFrame({\"class_name\": list(class_f1_scores.keys()),\n",
        "                          \"f1-score\": list(class_f1_scores.values())}).sort_values(\"f1-score\", ascending=False)\n",
        "f1_scores"
      ],
      "metadata": {
        "id": "oEL8gzmkRVJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 25))\n",
        "scores = ax.barh(range(len(f1_scores)), f1_scores[\"f1-score\"].values)\n",
        "ax.set_yticks(range(len(f1_scores)))\n",
        "ax.set_yticklabels(list(f1_scores[\"class_name\"]))\n",
        "ax.set_xlabel(\"f1-score\")\n",
        "ax.set_title(\"F1-Scores for 10 Different Classes\")\n",
        "ax.invert_yaxis(); # reverse the order\n",
        "\n",
        "def autolabel(rects): # Modified version of: https://matplotlib.org/examples/api/barchart_demo.html\n",
        "  \"\"\"\n",
        "  Attach a text label above each bar displaying its height (it's value).\n",
        "  \"\"\"\n",
        "  for rect in rects:\n",
        "    width = rect.get_width()\n",
        "    ax.text(1.03*width, rect.get_y() + rect.get_height()/1.5,\n",
        "            f\"{width:.2f}\",\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "autolabel(scores)"
      ],
      "metadata": {
        "id": "C7wL4YnvRVCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ],
      "metadata": {
        "id": "vTx6_m-jWoDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import load_and_prep_image\n",
        "# Make preds on a series of random images\n",
        "import os\n",
        "import random\n",
        "\n",
        "plt.figure(figsize=(17, 10))\n",
        "for i in range(3):\n",
        "  # Choose a random image from a random class\n",
        "  class_name = random.choice(train_classes)\n",
        "  filename = random.choice(os.listdir(test_dir_10_percent + \"/\" + class_name))\n",
        "  filepath = test_dir_10_percent +\"/\" + class_name + \"/\" + filename\n",
        "  print(filepath)\n",
        "\n",
        "  # Load the image and make predictions\n",
        "  img = load_and_prep_image(filepath, scale=False) # don't scale images for EfficientNet predictions\n",
        "  pred_prob = model.predict(tf.expand_dims(img, axis=0)) # model accepts tensors of shape [None, 224, 224, 3]\n",
        "  pred_class = train_classes[pred_prob.argmax()] # find the predicted class\n",
        "\n",
        "  # Plot the image(s)\n",
        "  plt.subplot(1, 3, i+1)\n",
        "  plt.imshow(img/255.)\n",
        "  if class_name == pred_class: # Change the color of text based on whether prediction is right or wrong\n",
        "    title_color = \"g\"\n",
        "  else:\n",
        "    title_color = \"r\"\n",
        "  plt.title(f\"actual: {class_name}, pred: {pred_class}, prob: {pred_prob.max():.2f}\", c=title_color)\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "Zj1fP9KZRVAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PM6UfeURU-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "55FxCd0WRU8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xE6tHscGRU57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cSaZteUYRU3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}