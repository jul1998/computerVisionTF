# -*- coding: utf-8 -*-
"""Emotions CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YlB7O615GMCgT_fOq9M1AYLUZ9FiOAG-
"""

# https://www.kaggle.com/datasets/nelgiriyewithana/emotions/code
# https://www.kaggle.com/code/alkidiarete/emosion-cnn-roc-0-99

!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d nelgiriyewithana/emotions

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py
from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir, calculate_results

! wget https://raw.githubusercontent.com/jul1998/tensorflow_projects/main/scripts_for_tensorflow.py

from scripts_for_tensorflow import *

unzip_data('/content/emotions.zip')

import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras import models,layers
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sklearn
import os

df = pd.read_csv('text.csv')

#df = df.drop(columns=['Unnamed: 0'])

df.info()

# Count nulls
#print(df.isnull().sum())

# Check duplicates
#print(df.duplicated().sum())

# Feature enginering

df1 = df.copy()

unique_review = df1['text'].unique()
#print(unique_review)

# Process text
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

df1['cleaned_text'] = df1['text']
df1['cleaned_text'] = df1['cleaned_text'].apply(lambda x: text_preprocessing(x))

df1['cleaned_text'] = df1['cleaned_text'].apply(lambda x: remove_stopwords(x))

df1['cleaned_text'] = df1['cleaned_text'].apply(lambda x: lemmatize_text(x))

print(df1['cleaned_text'])

# Label

labels = df1.label.value_counts()
print(labels)

# Mappling label

mapping = {
    0: 'sadness',
    1: 'joy',
    2: 'love',
    3: 'anger',
    4: 'fear',
    5: 'surprise'
}

df1['Emotion'] = df1['label'].map(mapping)

# Check emotion distribution
emotion_distribution = df1.Emotion.value_counts()
print(emotion_distribution)


# Modelling

df2 = df1.copy()

X_train, X_test, y_train, y_test = train_test_split(df2['cleaned_text'], df2['label'], test_size=0.2, random_state=42)

# CNN
# Tokenization
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=50000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index
#print(word_index)

# Padding

from tensorflow.keras.preprocessing.sequence import pad_sequences

X_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=100, padding='post', truncating='post')
X_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=100, padding='post', truncating='post')

# Model

model = models.Sequential([
    layers.Embedding(input_dim=50000, output_dim=16),
    layers.Conv1D(128, 5, activation='relu'),
    layers.GlobalMaxPooling1D(),
    layers.Dense(64, activation='relu'),
    layers.Dense(6, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Early Stopping Callback
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

history = model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_test_padded, y_test), callbacks=[early_stopping])

model.summary()

from keras.utils import plot_model
plot_model(model, to_file='model.png')

# prompt: plot loss

plot_loss_curves(history)

# prompt: evaluate model and get loos and accuracy

loss, accuracy = model.evaluate(X_test_padded, y_test)

print('Test loss:', loss)
print('Test accuracy:', accuracy)

from sklearn.metrics import confusion_matrix
import seaborn as sns
y_pred = np.argmax(model.predict(X_test_padded), axis=1)

conf_mat = confusion_matrix(y_test, y_pred)

emotion_labels = [
    'sadness', 'joy', 'love', 'anger', 'fear', 'surprise'
]

plt.figure(figsize=(8,6))
sns.heatmap(
    conf_mat, annot=True, fmt='d', cmap='Greys',
    xticklabels=emotion_labels,
    yticklabels=emotion_labels
)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from sklearn.metrics import classification_report
target_names = ['Sadness', 'Joy', 'Love', 'Anger', 'Fear', 'Surprise']
report = classification_report(y_test, y_pred, target_names=target_names)
report

calculate_results(y_test,y_pred )

# prompt: test model 1 using a sample text

sample_text = 'I want to die'

sample_text_processed = text_preprocessing(sample_text)
sample_text_processed = remove_stopwords(sample_text_processed)
sample_text_processed = lemmatize_text(sample_text_processed)

# Tokenize sample text
sample_text = tokenizer.texts_to_sequences([sample_text_processed])

# Pad sample text
sample_text_padded = pad_sequences(sample_text, maxlen=100, padding='post', truncating='post')

# Predict emotion for sample text
predicted_emotion = model.predict(sample_text_padded)
print(predicted_emotion)

# Get the predicted emotion label
predicted_emotion_label = mapping[np.argmax(predicted_emotion)]
print(predicted_emotion_label)

# Print the predicted emotion
print(f'Predicted emotion for sample text: {predicted_emotion_label}')





